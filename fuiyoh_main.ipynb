{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egokof2Rpz74",
        "outputId": "e6873ee1-140e-45d2-b558-b01cc1f301da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing training data...\n",
            "Training the enhanced model...\n",
            "Epoch 1/150\n",
            "421/421 [==============================] - 19s 37ms/step - loss: 2.6429 - accuracy: 0.3242 - val_loss: 2.2395 - val_accuracy: 0.3466 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.9234 - accuracy: 0.4326 - val_loss: 1.6872 - val_accuracy: 0.4912 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.7215 - accuracy: 0.4884 - val_loss: 1.5129 - val_accuracy: 0.5558 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.5955 - accuracy: 0.5277 - val_loss: 1.4111 - val_accuracy: 0.5884 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.5078 - accuracy: 0.5567 - val_loss: 1.3276 - val_accuracy: 0.6209 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 1.4407 - accuracy: 0.5816 - val_loss: 1.2691 - val_accuracy: 0.6409 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.3922 - accuracy: 0.6016 - val_loss: 1.2125 - val_accuracy: 0.6639 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.3518 - accuracy: 0.6170 - val_loss: 1.1887 - val_accuracy: 0.6755 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.3171 - accuracy: 0.6315 - val_loss: 1.1559 - val_accuracy: 0.6943 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.2928 - accuracy: 0.6419 - val_loss: 1.1270 - val_accuracy: 0.6977 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.2771 - accuracy: 0.6516 - val_loss: 1.1026 - val_accuracy: 0.7097 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 1.2569 - accuracy: 0.6623 - val_loss: 1.0878 - val_accuracy: 0.7301 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 1.2451 - accuracy: 0.6695 - val_loss: 1.0697 - val_accuracy: 0.7355 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.2297 - accuracy: 0.6773 - val_loss: 1.0673 - val_accuracy: 0.7417 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.2210 - accuracy: 0.6812 - val_loss: 1.0465 - val_accuracy: 0.7443 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.2123 - accuracy: 0.6882 - val_loss: 1.0384 - val_accuracy: 0.7476 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.2073 - accuracy: 0.6920 - val_loss: 1.0377 - val_accuracy: 0.7505 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1996 - accuracy: 0.6967 - val_loss: 1.0036 - val_accuracy: 0.7643 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1890 - accuracy: 0.7001 - val_loss: 1.0087 - val_accuracy: 0.7679 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1884 - accuracy: 0.7045 - val_loss: 0.9904 - val_accuracy: 0.7765 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 1.1799 - accuracy: 0.7056 - val_loss: 0.9843 - val_accuracy: 0.7814 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1733 - accuracy: 0.7099 - val_loss: 0.9926 - val_accuracy: 0.7735 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1651 - accuracy: 0.7121 - val_loss: 0.9937 - val_accuracy: 0.7822 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1588 - accuracy: 0.7181 - val_loss: 0.9606 - val_accuracy: 0.7876 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1590 - accuracy: 0.7187 - val_loss: 0.9606 - val_accuracy: 0.7891 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1520 - accuracy: 0.7216 - val_loss: 0.9579 - val_accuracy: 0.7890 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1538 - accuracy: 0.7216 - val_loss: 0.9532 - val_accuracy: 0.7938 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1438 - accuracy: 0.7266 - val_loss: 0.9401 - val_accuracy: 0.7990 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 1.1441 - accuracy: 0.7267 - val_loss: 0.9429 - val_accuracy: 0.8037 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1414 - accuracy: 0.7264 - val_loss: 0.9501 - val_accuracy: 0.7955 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1366 - accuracy: 0.7299 - val_loss: 0.9330 - val_accuracy: 0.8083 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1373 - accuracy: 0.7317 - val_loss: 0.9316 - val_accuracy: 0.8037 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1275 - accuracy: 0.7342 - val_loss: 0.9280 - val_accuracy: 0.8022 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 1.1245 - accuracy: 0.7341 - val_loss: 0.9223 - val_accuracy: 0.8057 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 1.1236 - accuracy: 0.7360 - val_loss: 0.9228 - val_accuracy: 0.8088 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1267 - accuracy: 0.7356 - val_loss: 0.9133 - val_accuracy: 0.8124 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1201 - accuracy: 0.7375 - val_loss: 0.9182 - val_accuracy: 0.8065 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1213 - accuracy: 0.7376 - val_loss: 0.9249 - val_accuracy: 0.8030 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1182 - accuracy: 0.7386 - val_loss: 0.9240 - val_accuracy: 0.8056 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1126 - accuracy: 0.7414 - val_loss: 0.9218 - val_accuracy: 0.8105 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1098 - accuracy: 0.7421 - val_loss: 0.8994 - val_accuracy: 0.8180 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1106 - accuracy: 0.7416 - val_loss: 0.9049 - val_accuracy: 0.8176 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 1.1070 - accuracy: 0.7440 - val_loss: 0.8940 - val_accuracy: 0.8206 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1044 - accuracy: 0.7446 - val_loss: 0.9117 - val_accuracy: 0.8113 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1038 - accuracy: 0.7442 - val_loss: 0.8986 - val_accuracy: 0.8167 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1083 - accuracy: 0.7452 - val_loss: 0.8993 - val_accuracy: 0.8150 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1006 - accuracy: 0.7482 - val_loss: 0.8949 - val_accuracy: 0.8217 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 1.1040 - accuracy: 0.7468 - val_loss: 0.9044 - val_accuracy: 0.8207 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.9412 - accuracy: 0.7983 - val_loss: 0.7307 - val_accuracy: 0.8731 - lr: 2.0000e-04\n",
            "Epoch 50/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.8561 - accuracy: 0.8221 - val_loss: 0.6843 - val_accuracy: 0.8828 - lr: 2.0000e-04\n",
            "Epoch 51/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.8178 - accuracy: 0.8296 - val_loss: 0.6560 - val_accuracy: 0.8873 - lr: 2.0000e-04\n",
            "Epoch 52/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.7924 - accuracy: 0.8341 - val_loss: 0.6347 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
            "Epoch 53/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.7682 - accuracy: 0.8394 - val_loss: 0.6105 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
            "Epoch 54/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.7479 - accuracy: 0.8436 - val_loss: 0.5947 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
            "Epoch 55/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.7301 - accuracy: 0.8471 - val_loss: 0.5793 - val_accuracy: 0.9032 - lr: 2.0000e-04\n",
            "Epoch 56/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.7164 - accuracy: 0.8489 - val_loss: 0.5701 - val_accuracy: 0.9049 - lr: 2.0000e-04\n",
            "Epoch 57/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.7041 - accuracy: 0.8514 - val_loss: 0.5598 - val_accuracy: 0.9054 - lr: 2.0000e-04\n",
            "Epoch 58/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.6905 - accuracy: 0.8534 - val_loss: 0.5459 - val_accuracy: 0.9095 - lr: 2.0000e-04\n",
            "Epoch 59/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6822 - accuracy: 0.8555 - val_loss: 0.5348 - val_accuracy: 0.9104 - lr: 2.0000e-04\n",
            "Epoch 60/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6749 - accuracy: 0.8562 - val_loss: 0.5255 - val_accuracy: 0.9140 - lr: 2.0000e-04\n",
            "Epoch 61/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6639 - accuracy: 0.8592 - val_loss: 0.5179 - val_accuracy: 0.9129 - lr: 2.0000e-04\n",
            "Epoch 62/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6538 - accuracy: 0.8594 - val_loss: 0.5125 - val_accuracy: 0.9131 - lr: 2.0000e-04\n",
            "Epoch 63/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6465 - accuracy: 0.8630 - val_loss: 0.5051 - val_accuracy: 0.9145 - lr: 2.0000e-04\n",
            "Epoch 64/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.6386 - accuracy: 0.8643 - val_loss: 0.4935 - val_accuracy: 0.9167 - lr: 2.0000e-04\n",
            "Epoch 65/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6360 - accuracy: 0.8625 - val_loss: 0.4911 - val_accuracy: 0.9178 - lr: 2.0000e-04\n",
            "Epoch 66/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6271 - accuracy: 0.8648 - val_loss: 0.4797 - val_accuracy: 0.9245 - lr: 2.0000e-04\n",
            "Epoch 67/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6214 - accuracy: 0.8661 - val_loss: 0.4795 - val_accuracy: 0.9194 - lr: 2.0000e-04\n",
            "Epoch 68/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6143 - accuracy: 0.8680 - val_loss: 0.4791 - val_accuracy: 0.9234 - lr: 2.0000e-04\n",
            "Epoch 69/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6106 - accuracy: 0.8677 - val_loss: 0.4705 - val_accuracy: 0.9223 - lr: 2.0000e-04\n",
            "Epoch 70/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6062 - accuracy: 0.8693 - val_loss: 0.4625 - val_accuracy: 0.9250 - lr: 2.0000e-04\n",
            "Epoch 71/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.6052 - accuracy: 0.8685 - val_loss: 0.4625 - val_accuracy: 0.9251 - lr: 2.0000e-04\n",
            "Epoch 72/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5964 - accuracy: 0.8723 - val_loss: 0.4611 - val_accuracy: 0.9203 - lr: 2.0000e-04\n",
            "Epoch 73/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5950 - accuracy: 0.8712 - val_loss: 0.4555 - val_accuracy: 0.9280 - lr: 2.0000e-04\n",
            "Epoch 74/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5904 - accuracy: 0.8722 - val_loss: 0.4514 - val_accuracy: 0.9269 - lr: 2.0000e-04\n",
            "Epoch 75/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5849 - accuracy: 0.8738 - val_loss: 0.4455 - val_accuracy: 0.9280 - lr: 2.0000e-04\n",
            "Epoch 76/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5849 - accuracy: 0.8726 - val_loss: 0.4389 - val_accuracy: 0.9291 - lr: 2.0000e-04\n",
            "Epoch 77/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5774 - accuracy: 0.8761 - val_loss: 0.4407 - val_accuracy: 0.9271 - lr: 2.0000e-04\n",
            "Epoch 78/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5785 - accuracy: 0.8740 - val_loss: 0.4332 - val_accuracy: 0.9317 - lr: 2.0000e-04\n",
            "Epoch 79/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5741 - accuracy: 0.8768 - val_loss: 0.4385 - val_accuracy: 0.9273 - lr: 2.0000e-04\n",
            "Epoch 80/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5706 - accuracy: 0.8767 - val_loss: 0.4350 - val_accuracy: 0.9287 - lr: 2.0000e-04\n",
            "Epoch 81/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5685 - accuracy: 0.8771 - val_loss: 0.4296 - val_accuracy: 0.9297 - lr: 2.0000e-04\n",
            "Epoch 82/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5638 - accuracy: 0.8771 - val_loss: 0.4255 - val_accuracy: 0.9304 - lr: 2.0000e-04\n",
            "Epoch 83/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5612 - accuracy: 0.8781 - val_loss: 0.4208 - val_accuracy: 0.9331 - lr: 2.0000e-04\n",
            "Epoch 84/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5593 - accuracy: 0.8785 - val_loss: 0.4247 - val_accuracy: 0.9293 - lr: 2.0000e-04\n",
            "Epoch 85/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5557 - accuracy: 0.8798 - val_loss: 0.4238 - val_accuracy: 0.9307 - lr: 2.0000e-04\n",
            "Epoch 86/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5584 - accuracy: 0.8776 - val_loss: 0.4210 - val_accuracy: 0.9307 - lr: 2.0000e-04\n",
            "Epoch 87/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5517 - accuracy: 0.8808 - val_loss: 0.4132 - val_accuracy: 0.9347 - lr: 2.0000e-04\n",
            "Epoch 88/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5516 - accuracy: 0.8804 - val_loss: 0.4179 - val_accuracy: 0.9310 - lr: 2.0000e-04\n",
            "Epoch 89/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5478 - accuracy: 0.8807 - val_loss: 0.4115 - val_accuracy: 0.9329 - lr: 2.0000e-04\n",
            "Epoch 90/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5464 - accuracy: 0.8817 - val_loss: 0.4124 - val_accuracy: 0.9329 - lr: 2.0000e-04\n",
            "Epoch 91/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5476 - accuracy: 0.8812 - val_loss: 0.4121 - val_accuracy: 0.9314 - lr: 2.0000e-04\n",
            "Epoch 92/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5405 - accuracy: 0.8829 - val_loss: 0.4091 - val_accuracy: 0.9322 - lr: 2.0000e-04\n",
            "Epoch 93/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5399 - accuracy: 0.8820 - val_loss: 0.4029 - val_accuracy: 0.9350 - lr: 2.0000e-04\n",
            "Epoch 94/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5370 - accuracy: 0.8837 - val_loss: 0.4019 - val_accuracy: 0.9358 - lr: 2.0000e-04\n",
            "Epoch 95/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5357 - accuracy: 0.8836 - val_loss: 0.3981 - val_accuracy: 0.9361 - lr: 2.0000e-04\n",
            "Epoch 96/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5339 - accuracy: 0.8822 - val_loss: 0.4030 - val_accuracy: 0.9325 - lr: 2.0000e-04\n",
            "Epoch 97/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5361 - accuracy: 0.8836 - val_loss: 0.3965 - val_accuracy: 0.9374 - lr: 2.0000e-04\n",
            "Epoch 98/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5319 - accuracy: 0.8855 - val_loss: 0.3990 - val_accuracy: 0.9351 - lr: 2.0000e-04\n",
            "Epoch 99/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5330 - accuracy: 0.8842 - val_loss: 0.3927 - val_accuracy: 0.9370 - lr: 2.0000e-04\n",
            "Epoch 100/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5295 - accuracy: 0.8854 - val_loss: 0.3941 - val_accuracy: 0.9376 - lr: 2.0000e-04\n",
            "Epoch 101/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5277 - accuracy: 0.8848 - val_loss: 0.3953 - val_accuracy: 0.9381 - lr: 2.0000e-04\n",
            "Epoch 102/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5218 - accuracy: 0.8876 - val_loss: 0.3951 - val_accuracy: 0.9372 - lr: 2.0000e-04\n",
            "Epoch 103/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5262 - accuracy: 0.8851 - val_loss: 0.3957 - val_accuracy: 0.9342 - lr: 2.0000e-04\n",
            "Epoch 104/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5259 - accuracy: 0.8852 - val_loss: 0.3917 - val_accuracy: 0.9381 - lr: 2.0000e-04\n",
            "Epoch 105/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5204 - accuracy: 0.8884 - val_loss: 0.3914 - val_accuracy: 0.9361 - lr: 2.0000e-04\n",
            "Epoch 106/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5195 - accuracy: 0.8875 - val_loss: 0.3905 - val_accuracy: 0.9374 - lr: 2.0000e-04\n",
            "Epoch 107/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5204 - accuracy: 0.8874 - val_loss: 0.3882 - val_accuracy: 0.9375 - lr: 2.0000e-04\n",
            "Epoch 108/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5184 - accuracy: 0.8878 - val_loss: 0.3913 - val_accuracy: 0.9360 - lr: 2.0000e-04\n",
            "Epoch 109/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5184 - accuracy: 0.8878 - val_loss: 0.3932 - val_accuracy: 0.9344 - lr: 2.0000e-04\n",
            "Epoch 110/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5172 - accuracy: 0.8875 - val_loss: 0.3862 - val_accuracy: 0.9393 - lr: 2.0000e-04\n",
            "Epoch 111/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5173 - accuracy: 0.8874 - val_loss: 0.3870 - val_accuracy: 0.9369 - lr: 2.0000e-04\n",
            "Epoch 112/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5188 - accuracy: 0.8882 - val_loss: 0.3822 - val_accuracy: 0.9399 - lr: 2.0000e-04\n",
            "Epoch 113/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5136 - accuracy: 0.8890 - val_loss: 0.3816 - val_accuracy: 0.9403 - lr: 2.0000e-04\n",
            "Epoch 114/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5134 - accuracy: 0.8887 - val_loss: 0.3780 - val_accuracy: 0.9399 - lr: 2.0000e-04\n",
            "Epoch 115/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5116 - accuracy: 0.8900 - val_loss: 0.3864 - val_accuracy: 0.9357 - lr: 2.0000e-04\n",
            "Epoch 116/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5110 - accuracy: 0.8901 - val_loss: 0.3783 - val_accuracy: 0.9404 - lr: 2.0000e-04\n",
            "Epoch 117/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5126 - accuracy: 0.8887 - val_loss: 0.3854 - val_accuracy: 0.9371 - lr: 2.0000e-04\n",
            "Epoch 118/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5093 - accuracy: 0.8892 - val_loss: 0.3770 - val_accuracy: 0.9400 - lr: 2.0000e-04\n",
            "Epoch 119/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5107 - accuracy: 0.8894 - val_loss: 0.3781 - val_accuracy: 0.9385 - lr: 2.0000e-04\n",
            "Epoch 120/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5075 - accuracy: 0.8903 - val_loss: 0.3775 - val_accuracy: 0.9401 - lr: 2.0000e-04\n",
            "Epoch 121/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5049 - accuracy: 0.8905 - val_loss: 0.3795 - val_accuracy: 0.9393 - lr: 2.0000e-04\n",
            "Epoch 122/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5067 - accuracy: 0.8907 - val_loss: 0.3750 - val_accuracy: 0.9398 - lr: 2.0000e-04\n",
            "Epoch 123/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5105 - accuracy: 0.8892 - val_loss: 0.3776 - val_accuracy: 0.9403 - lr: 2.0000e-04\n",
            "Epoch 124/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5067 - accuracy: 0.8900 - val_loss: 0.3711 - val_accuracy: 0.9427 - lr: 2.0000e-04\n",
            "Epoch 125/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5034 - accuracy: 0.8921 - val_loss: 0.3774 - val_accuracy: 0.9408 - lr: 2.0000e-04\n",
            "Epoch 126/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5032 - accuracy: 0.8925 - val_loss: 0.3793 - val_accuracy: 0.9374 - lr: 2.0000e-04\n",
            "Epoch 127/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.5044 - accuracy: 0.8925 - val_loss: 0.3763 - val_accuracy: 0.9388 - lr: 2.0000e-04\n",
            "Epoch 128/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.4993 - accuracy: 0.8929 - val_loss: 0.3728 - val_accuracy: 0.9417 - lr: 2.0000e-04\n",
            "Epoch 129/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.5012 - accuracy: 0.8924 - val_loss: 0.3771 - val_accuracy: 0.9385 - lr: 2.0000e-04\n",
            "Epoch 130/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.4566 - accuracy: 0.9081 - val_loss: 0.3437 - val_accuracy: 0.9511 - lr: 4.0000e-05\n",
            "Epoch 131/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.4325 - accuracy: 0.9151 - val_loss: 0.3366 - val_accuracy: 0.9523 - lr: 4.0000e-05\n",
            "Epoch 132/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.4251 - accuracy: 0.9170 - val_loss: 0.3324 - val_accuracy: 0.9537 - lr: 4.0000e-05\n",
            "Epoch 133/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.4171 - accuracy: 0.9206 - val_loss: 0.3319 - val_accuracy: 0.9534 - lr: 4.0000e-05\n",
            "Epoch 134/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.4126 - accuracy: 0.9211 - val_loss: 0.3277 - val_accuracy: 0.9536 - lr: 4.0000e-05\n",
            "Epoch 135/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.4062 - accuracy: 0.9222 - val_loss: 0.3255 - val_accuracy: 0.9555 - lr: 4.0000e-05\n",
            "Epoch 136/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3997 - accuracy: 0.9244 - val_loss: 0.3264 - val_accuracy: 0.9539 - lr: 4.0000e-05\n",
            "Epoch 137/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.4011 - accuracy: 0.9242 - val_loss: 0.3238 - val_accuracy: 0.9545 - lr: 4.0000e-05\n",
            "Epoch 138/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3999 - accuracy: 0.9248 - val_loss: 0.3210 - val_accuracy: 0.9558 - lr: 4.0000e-05\n",
            "Epoch 139/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3972 - accuracy: 0.9250 - val_loss: 0.3192 - val_accuracy: 0.9551 - lr: 4.0000e-05\n",
            "Epoch 140/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3917 - accuracy: 0.9266 - val_loss: 0.3173 - val_accuracy: 0.9557 - lr: 4.0000e-05\n",
            "Epoch 141/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3919 - accuracy: 0.9271 - val_loss: 0.3180 - val_accuracy: 0.9553 - lr: 4.0000e-05\n",
            "Epoch 142/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3881 - accuracy: 0.9266 - val_loss: 0.3167 - val_accuracy: 0.9549 - lr: 4.0000e-05\n",
            "Epoch 143/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3877 - accuracy: 0.9271 - val_loss: 0.3143 - val_accuracy: 0.9556 - lr: 4.0000e-05\n",
            "Epoch 144/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3883 - accuracy: 0.9257 - val_loss: 0.3136 - val_accuracy: 0.9562 - lr: 4.0000e-05\n",
            "Epoch 145/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.3871 - accuracy: 0.9262 - val_loss: 0.3112 - val_accuracy: 0.9564 - lr: 4.0000e-05\n",
            "Epoch 146/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3804 - accuracy: 0.9290 - val_loss: 0.3117 - val_accuracy: 0.9562 - lr: 4.0000e-05\n",
            "Epoch 147/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3786 - accuracy: 0.9295 - val_loss: 0.3099 - val_accuracy: 0.9561 - lr: 4.0000e-05\n",
            "Epoch 148/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.3784 - accuracy: 0.9289 - val_loss: 0.3086 - val_accuracy: 0.9568 - lr: 4.0000e-05\n",
            "Epoch 149/150\n",
            "421/421 [==============================] - 14s 34ms/step - loss: 0.3780 - accuracy: 0.9296 - val_loss: 0.3071 - val_accuracy: 0.9579 - lr: 4.0000e-05\n",
            "Epoch 150/150\n",
            "421/421 [==============================] - 14s 33ms/step - loss: 0.3766 - accuracy: 0.9299 - val_loss: 0.3060 - val_accuracy: 0.9580 - lr: 4.0000e-05\n",
            "841/841 [==============================] - 3s 4ms/step - loss: 0.3060 - accuracy: 0.9580\n",
            "Validation accuracy: 0.9580\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LeakyReLU, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Enhanced Hyperparameters from the upper model\n",
        "best_params = {\n",
        "    'neurons': [256 ,512 ,1024 ,512, 256],# Layer sizes\n",
        "    'dropout_rate': 0.4,  # Increased dropout\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 256,  # Adjusted batch size\n",
        "    'epochs': 150,  # Increased epochs\n",
        "    'l2_lambda': 0.0001  # L2 regularization factor\n",
        "}\n",
        "\n",
        "# Step 1: Preprocess Data using TF-IDF\n",
        "def preprocess_data_tfidf(df, ohe_encoder=None, label_encoder=None, tfidf_vectorizer=None, is_train=True):\n",
        "    # One-Hot Encoding\n",
        "    ohe_columns = ['DRUGTYPE', 'Drug_high_status', 'Drug_Status']\n",
        "    if ohe_encoder is None:\n",
        "        ohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        ohe_features = ohe_encoder.fit_transform(df[ohe_columns])\n",
        "    else:\n",
        "        ohe_features = ohe_encoder.transform(df[ohe_columns])\n",
        "\n",
        "    # TF-IDF Vectorization for text columns\n",
        "    text_columns = ['Disease_of_highest_status', 'BIOCLASS', 'TARGNAME', 'SYNONYMS', 'FUNCTION', 'Disease']\n",
        "    combined_text = df[text_columns].astype(str).apply(lambda row: ' '.join(row.values), axis=1)\n",
        "\n",
        "    if tfidf_vectorizer is None:\n",
        "        tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        tfidf_features = tfidf_vectorizer.fit_transform(combined_text).toarray()\n",
        "    else:\n",
        "        tfidf_features = tfidf_vectorizer.transform(combined_text).toarray()\n",
        "\n",
        "    # Combine all features\n",
        "    X = np.hstack((ohe_features, tfidf_features))\n",
        "\n",
        "    if is_train:\n",
        "        # Label Encoding for target\n",
        "        if label_encoder is None:\n",
        "            label_encoder = LabelEncoder()\n",
        "            y = label_encoder.fit_transform(df['Target_Status'])\n",
        "        else:\n",
        "            y = label_encoder.transform(df['Target_Status'])\n",
        "        return X, y, ohe_encoder, label_encoder, tfidf_vectorizer\n",
        "    else:\n",
        "        return X\n",
        "\n",
        "# Step 2: Load and preprocess training data\n",
        "print(\"Loading and preprocessing training data...\")\n",
        "train_data = pd.read_csv('train.csv')\n",
        "X, y, ohe_encoder, label_encoder, tfidf_vectorizer = preprocess_data_tfidf(train_data)\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Step 4: Define the enhanced TensorFlow model with split branches\n",
        "def create_enhanced_model(input_dim, num_classes, neurons, dropout_rate, l2_lambda):\n",
        "    input_layer = Input(shape=(input_dim,), name='input_layer')\n",
        "\n",
        "    # Split the input into two branches\n",
        "    branch1 = Dense(neurons[0], kernel_regularizer=l2(l2_lambda))(input_layer)\n",
        "    branch1 = LeakyReLU(alpha=0.2)(branch1)\n",
        "    branch1 = BatchNormalization()(branch1)\n",
        "    branch1 = Dropout(dropout_rate)(branch1)\n",
        "\n",
        "    branch2 = Dense(neurons[0] // 2, kernel_regularizer=l2(l2_lambda))(input_layer)\n",
        "    branch2 = LeakyReLU(alpha=0.2)(branch2)\n",
        "    branch2 = BatchNormalization()(branch2)\n",
        "    branch2 = Dropout(dropout_rate)(branch2)\n",
        "\n",
        "    # Merge branches\n",
        "    x = Concatenate()([branch1, branch2])\n",
        "\n",
        "    for i in range(1, len(neurons)):\n",
        "        x = Dense(neurons[i], kernel_regularizer=l2(l2_lambda))(x)\n",
        "        x = LeakyReLU(alpha=0.2)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Step 5: Compile and train the model\n",
        "print(\"Training the enhanced model...\")\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "model = create_enhanced_model(input_dim, num_classes, best_params['neurons'], best_params['dropout_rate'], best_params['l2_lambda'])\n",
        "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=best_params['epochs'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation accuracy: {val_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLarHOm3zAK1",
        "outputId": "ae110c52-846d-49f3-ca7a-ac1ed5205a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions...\n",
            "1802/1802 [==============================] - 7s 4ms/step\n",
            "Saving predictions...\n",
            "Predictions saved to 'predictions.csv'\n"
          ]
        }
      ],
      "source": [
        "test_data = pd.read_csv('test.csv')\n",
        "X_test = preprocess_data_tfidf(test_data, ohe_encoder, label_encoder, tfidf_vectorizer, is_train=False)\n",
        "\n",
        "print(\"Making predictions...\")\n",
        "test_predictions = model.predict(X_test)\n",
        "test_predictions_classes = label_encoder.inverse_transform(np.argmax(test_predictions, axis=1))\n",
        "\n",
        "# Step 8: Save predictions\n",
        "print(\"Saving predictions...\")\n",
        "predictions_df = pd.DataFrame({\n",
        "    'ID': test_data['ID'],\n",
        "    'Predictions': test_predictions_classes\n",
        "})\n",
        "predictions_df.to_csv('predictions5layered.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'predictions.csv'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
